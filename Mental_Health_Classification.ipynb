{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOw/vbXGNtONOOIdN6YUl+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeshmin-shrestha/Mental-Health-Classification/blob/main/Mental_Health_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTDoGrzV80Ho"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "nltk.download(['punkt', 'stopwords', 'wordnet'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/sentiment-analysis-for-mental-health-Combined%20Data (1).csv')\n",
        "df"
      ],
      "metadata": {
        "id": "xL3aQ5dN9elL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "zfdaY3av9sDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "id": "YlfqVQC49xbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "YTm8v9UI91T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop redundant column\n",
        "df = df[['statement', 'status']]  # Remove Unnamed: 0"
      ],
      "metadata": {
        "id": "zGtRhG_w97p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "J93-ZNff-u2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "jHDqS5F2-xRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "e5gdZpT9-ycg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['status'].value_counts())"
      ],
      "metadata": {
        "id": "PmUK_lpg-0wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(data=df, x='status', order=df['status'].value_counts().index, palette='viridis')\n",
        "plt.title('Class Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bA87j0qo-2tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tSqojwL2-7e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='status', order=df['status'].value_counts().index, palette='viridis')\n",
        "plt.title('Class Distribution in Dataset')\n",
        "plt.xlabel('Mental Health Status')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Print percentages for report\n",
        "print(df['status'].value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "id": "hcTBELN6_U1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column for text length\n",
        "df['text_length'] = df['statement'].apply(len)\n",
        "\n",
        "# Histogram of text lengths\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['text_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Statement Lengths')\n",
        "plt.xlabel('Length (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Stats for report\n",
        "print(df['text_length'].describe())"
      ],
      "metadata": {
        "id": "5j3-qEbD_Xps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate word cloud\n",
        "def generate_wordcloud(class_name):\n",
        "    text = ' '.join(df[df['status'] == class_name]['statement'].dropna())\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=50).generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Word Cloud for {class_name} Class')\n",
        "    plt.show()\n",
        "\n",
        "# Generate for key classes\n",
        "generate_wordcloud('Anxiety')\n",
        "\n"
      ],
      "metadata": {
        "id": "ur50sQPL_gXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud('Depression')\n"
      ],
      "metadata": {
        "id": "ddgAcVwd_x9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud('Suicidal')\n"
      ],
      "metadata": {
        "id": "js9azta-_76V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud('Normal')\n"
      ],
      "metadata": {
        "id": "Xaxq866F__Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud('Stress')\n"
      ],
      "metadata": {
        "id": "NnkNpt4qAAst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud('Bipolar')\n"
      ],
      "metadata": {
        "id": "S28SvKBeACwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud('Personality disorder')"
      ],
      "metadata": {
        "id": "G6ZM1x2AADoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required tokenizer packages\n",
        "nltk.download('punkt')          # Original punkt (fallback)\n",
        "nltk.download('punkt_tab')      # New required one\n",
        "\n",
        "# Also ensure others are there\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "L9v_TEEeAikN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):  # Handle any lingering NaNs\n",
        "        return ''\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special chars, punctuation, numbers\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatize and remove stops\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply to dataset\n",
        "df['cleaned_statement'] = df['statement'].apply(preprocess_text)\n",
        "\n",
        "# Check sample\n",
        "print(df[['statement', 'cleaned_statement']].head(10))"
      ],
      "metadata": {
        "id": "2Bg9_ciiAFju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size before and after preprocessing\n",
        "original_vocab = set(' '.join(df['statement']).lower().split())\n",
        "cleaned_vocab = set(' '.join(df['cleaned_statement']).split())\n",
        "\n",
        "print(f\"Original vocabulary size (unique words): {len(original_vocab):,}\")\n",
        "print(f\"Cleaned vocabulary size (unique words): {len(cleaned_vocab):,}\")\n",
        "print(f\"Reduction: {len(original_vocab) - len(cleaned_vocab):,} words ({(1 - len(cleaned_vocab)/len(original_vocab))*100:.1f}% reduction)\")"
      ],
      "metadata": {
        "id": "6Qc0hpp0AZMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# Top N words per class\n",
        "def top_words_per_class(class_name, n=10):\n",
        "    class_text = ' '.join(df[df['status'] == class_name]['cleaned_statement'])\n",
        "    words = class_text.split()\n",
        "    freq = Counter(words)\n",
        "    print(f\"\\nTop {n} words in '{class_name}' class:\")\n",
        "    for word, count in freq.most_common(n):\n",
        "        print(f\"  {word}: {count}\")\n",
        "\n",
        "# Run for all classes\n",
        "classes = df['status'].unique()\n",
        "for cls in classes:\n",
        "    top_words_per_class(cls, n=10)"
      ],
      "metadata": {
        "id": "nTqXJv6nA_2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['cleaned_statement']\n",
        "y = df['status']\n",
        "\n",
        "# 60% train, 20% validation, 20% test (stratified to keep class ratios)\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, stratify=y_train_full, random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(X_train):,} | Val size: {len(X_val):,} | Test size: {len(X_test):,}\")\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "print(y_train.value_counts())"
      ],
      "metadata": {
        "id": "7InT9Cn9BJRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use reasonable max_features to avoid memory issues and overfitting\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))  # unigrams + bigrams\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF vocabulary size: {len(vectorizer.get_feature_names_out()):,}\")\n",
        "print(f\"Shape: {X_train_tfidf.shape}\")"
      ],
      "metadata": {
        "id": "QWms8DwKBaw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper function for evaluation\n",
        "def evaluate_model(model, X_data, y_true, dataset_name=\"Validation\"):\n",
        "    preds = model.predict(X_data)\n",
        "    print(f\"\\n=== {dataset_name} Results ===\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, preds):.4f}\")\n",
        "    print(f\"Macro F1-score: {f1_score(y_true, preds, average='macro'):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, preds))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, preds, labels=model.classes_)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
        "    plt.ylabel('True')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GK97B7_HBkd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# class_weight='balanced' is crucial for your imbalanced data\n",
        "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate on validation\n",
        "evaluate_model(lr_model, X_val_tfidf, y_val, \"Logistic Regression - Validation\")"
      ],
      "metadata": {
        "id": "JKlB2PxiBrOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_model = LinearSVC(class_weight='balanced', max_iter=2000, random_state=42)\n",
        "\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "evaluate_model(svm_model, X_val_tfidf, y_val, \"SVM - Validation\")"
      ],
      "metadata": {
        "id": "ZjAA50rRBwEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick comparison\n",
        "val_preds_lr = lr_model.predict(X_val_tfidf)\n",
        "val_preds_svm = svm_model.predict(X_val_tfidf)\n",
        "\n",
        "print(\"Validation Macro F1 Comparison:\")\n",
        "print(f\"Logistic Regression: {f1_score(y_val, val_preds_lr, average='macro'):.4f}\")\n",
        "print(f\"SVM: {f1_score(y_val, val_preds_svm, average='macro'):.4f}\")"
      ],
      "metadata": {
        "id": "HFg-0qJxByCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample predictions\n",
        "sample_texts = X_val.head(5)\n",
        "sample_true = y_val.head(5).values\n",
        "sample_preds_lr = lr_model.predict(vectorizer.transform(sample_texts))\n",
        "sample_preds_svm = svm_model.predict(vectorizer.transform(sample_texts))\n",
        "\n",
        "print(\"Sample Predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nText: {df.loc[sample_texts.index[i], 'statement']}\")\n",
        "    print(f\"True: {sample_true[i]} | LR: {sample_preds_lr[i]} | SVM: {sample_preds_svm[i]}\")"
      ],
      "metadata": {
        "id": "ptdeQQ9RB3ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on test set using best model (Logistic Regression)\n",
        "print(\"=== FINAL TEST SET RESULTS (Logistic Regression) ===\")\n",
        "evaluate_model(lr_model, X_test_tfidf, y_test, \"Logistic Regression - Test Set\")"
      ],
      "metadata": {
        "id": "UsDFNPMvCF-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch accelerate -q\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Label mapping\n",
        "unique_labels = sorted(df['status'].unique())  # Sort for consistency\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "num_labels = len(label2id)\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_df = pd.DataFrame({'text': X_train, 'label': y_train.map(label2id)})\n",
        "val_df = pd.DataFrame({'text': X_val, 'label': y_val.map(label2id)})\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = train_ds.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_ds.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# Model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Compute metrics function (accuracy + macro F1)\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    return {\"accuracy\": acc, \"macro_f1\": macro_f1}\n",
        "\n",
        "# Updated TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bert_results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",               # Disable Weight and Biases\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "tgedtaDBCXIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "# Validation predictions\n",
        "val_preds_lr = lr_model.predict(X_val_tfidf)\n",
        "val_preds_svm = svm_model.predict(X_val_tfidf)\n",
        "\n",
        "val_macro_f1_lr = f1_score(y_val, val_preds_lr, average='macro')\n",
        "val_macro_f1_svm = f1_score(y_val, val_preds_svm, average='macro')\n",
        "\n",
        "# Test predictions\n",
        "test_preds_lr = lr_model.predict(X_test_tfidf)\n",
        "test_preds_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "test_macro_f1_lr = f1_score(y_test, test_preds_lr, average='macro')\n",
        "test_macro_f1_svm = f1_score(y_test, test_preds_svm, average='macro')\n",
        "\n",
        "test_accuracy_lr = accuracy_score(y_test, test_preds_lr)\n",
        "test_accuracy_svm = accuracy_score(y_test, test_preds_svm)\n",
        "\n",
        "# Print full reports\n",
        "print(\"=== Logistic Regression - Test Set ===\")\n",
        "print(f\"Test Accuracy: {test_accuracy_lr:.4f}\")\n",
        "print(f\"Test Macro F1: {test_macro_f1_lr:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, test_preds_lr))\n",
        "\n",
        "print(\"\\n=== Linear SVM - Test Set ===\")\n",
        "print(f\"Test Accuracy: {test_accuracy_svm:.4f}\")\n",
        "print(f\"Test Macro F1: {test_macro_f1_svm:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, test_preds_svm))\n",
        "\n",
        "# Fully automatic comparison table\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(\"| Model               | Validation Macro F1 | Test Macro F1 | Test Accuracy |\")\n",
        "print(\"|---------------------|---------------------|---------------|---------------|\")\n",
        "print(f\"| Logistic Regression | {val_macro_f1_lr:.4f}             | {test_macro_f1_lr:.4f}        | {test_accuracy_lr:.4f}       |\")\n",
        "print(f\"| Linear SVM          | {val_macro_f1_svm:.4f}             | {test_macro_f1_svm:.4f}        | {test_accuracy_svm:.4f}       |\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "DkzVJSHtCi10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Evaluate on validation set (built-in from Trainer)\n",
        "print(\"=== BERT Validation Results ===\")\n",
        "val_results = trainer.evaluate()\n",
        "print(val_results)"
      ],
      "metadata": {
        "id": "NFjh65-RIQh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Prepare test set for evaluation\n",
        "test_df = pd.DataFrame({'text': X_test, 'label': y_test.map(label2id)})\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "tokenized_test = test_ds.map(tokenize_function, batched=True)\n",
        "tokenized_test.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\n=== BERT Test Set Results ===\")\n",
        "test_results = trainer.evaluate(tokenized_test)\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "VwkcGjJnItAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# COMPREHENSIVE MODEL COMPARISON TABLE\n",
        "# ==============================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL MODEL COMPARISON - ALL THREE MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get BERT metrics from evaluation\n",
        "bert_val_accuracy = val_results.get('eval_accuracy', 0)\n",
        "bert_val_macro_f1 = val_results.get('eval_macro_f1', 0)\n",
        "bert_test_accuracy = test_results.get('eval_accuracy', 0)\n",
        "bert_test_macro_f1 = test_results.get('eval_macro_f1', 0)\n",
        "\n",
        "# Create comparison dataframe\n",
        "import pandas as pd\n",
        "\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'Linear SVM', 'BERT (Fine-tuned)'],\n",
        "    'Validation Accuracy': [accuracy_score(y_val, val_preds_lr),\n",
        "                           accuracy_score(y_val, val_preds_svm),\n",
        "                           bert_val_accuracy],\n",
        "    'Test Accuracy': [test_accuracy_lr, test_accuracy_svm, bert_test_accuracy],\n",
        "    'Validation Macro F1': [val_macro_f1_lr, val_macro_f1_svm, bert_val_macro_f1],\n",
        "    'Test Macro F1': [test_macro_f1_lr, test_macro_f1_svm, bert_test_macro_f1],\n",
        "    'Training Time (est)': ['~1 min', '~2 min', '~30-40 min'],\n",
        "    'Inference Speed': ['Fastest', 'Fast', 'Slowest'],\n",
        "    'Interpretability': ['High (feature importance)', 'Medium', 'Low (black box)']\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE METRICS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df[['Model', 'Validation Accuracy', 'Test Accuracy',\n",
        "                     'Validation Macro F1', 'Test Macro F1']].to_string(index=False))\n",
        "\n",
        "# Visual comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VISUAL COMPARISON - BAR CHART\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create bar chart\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0,0].bar(comparison_df['Model'], comparison_df['Test Accuracy'], color=['blue', 'green', 'red'])\n",
        "axes[0,0].set_title('Test Accuracy Comparison')\n",
        "axes[0,0].set_ylabel('Accuracy')\n",
        "axes[0,0].set_ylim(0.5, 0.9)\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "# Add value labels\n",
        "for i, v in enumerate(comparison_df['Test Accuracy']):\n",
        "    axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "# Macro F1 comparison\n",
        "axes[0,1].bar(comparison_df['Model'], comparison_df['Test Macro F1'], color=['blue', 'green', 'red'])\n",
        "axes[0,1].set_title('Test Macro F1 Score Comparison')\n",
        "axes[0,1].set_ylabel('Macro F1')\n",
        "axes[0,1].set_ylim(0.5, 0.9)\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['Test Macro F1']):\n",
        "    axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "# Training vs Inference trade-off\n",
        "axes[1,0].bar(comparison_df['Model'], [1, 2, 30], color='orange')\n",
        "axes[1,0].set_title('Relative Training Time (minutes)')\n",
        "axes[1,0].set_ylabel('Minutes (approx)')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "training_times = [1, 2, 30]\n",
        "for i, v in enumerate(training_times):\n",
        "    axes[1,0].text(i, v + 1, f'{v} min', ha='center')\n",
        "\n",
        "# Performance on Minority Classes (example: Personality Disorder)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate F1 for each class for each model\n",
        "classes = sorted(y_test.unique())\n",
        "f1_by_class = {}\n",
        "\n",
        "for model_name, predictions in [('Logistic Regression', test_preds_lr),\n",
        "                                ('SVM', test_preds_svm)]:\n",
        "    f1_scores = []\n",
        "    for cls in classes:\n",
        "        f1 = f1_score(y_test == cls, predictions == cls)\n",
        "        f1_scores.append(f1)\n",
        "    f1_by_class[model_name] = f1_scores\n",
        "\n",
        "# For BERT, we need to get predictions first\n",
        "bert_test_preds = trainer.predict(tokenized_test)\n",
        "bert_pred_labels = np.argmax(bert_test_preds.predictions, axis=-1)\n",
        "bert_pred_classes = [id2label[i] for i in bert_pred_labels]\n",
        "\n",
        "f1_scores_bert = []\n",
        "for cls in classes:\n",
        "    f1 = f1_score(y_test == cls, np.array(bert_pred_classes) == cls)\n",
        "    f1_scores_bert.append(f1)\n",
        "f1_by_class['BERT'] = f1_scores_bert\n",
        "\n",
        "# Plot Personality Disorder (minority class) F1\n",
        "minority_idx = list(classes).index('Personality disorder')\n",
        "minority_f1 = [f1_by_class['Logistic Regression'][minority_idx],\n",
        "               f1_by_class['SVM'][minority_idx],\n",
        "               f1_by_class['BERT'][minority_idx]]\n",
        "\n",
        "axes[1,1].bar(comparison_df['Model'], minority_f1, color=['blue', 'green', 'red'])\n",
        "axes[1,1].set_title(\"F1 Score for 'Personality disorder' (Minority Class)\")\n",
        "axes[1,1].set_ylabel('F1 Score')\n",
        "axes[1,1].set_ylim(0, 1)\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(minority_f1):\n",
        "    axes[1,1].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==============================================\n",
        "# KEY FINDINGS SUMMARY\n",
        "# ==============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY FINDINGS & RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find best model by Macro F1 (most important for imbalanced data)\n",
        "best_model_idx = comparison_df['Test Macro F1'].idxmax()\n",
        "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
        "best_f1 = comparison_df.loc[best_model_idx, 'Test Macro F1']\n",
        "\n",
        "print(f\"1. BEST MODEL: {best_model_name} achieves the highest Macro F1 score ({best_f1:.4f})\")\n",
        "print(\"   - Macro F1 is the most important metric for imbalanced datasets\")\n",
        "print(\"   - It ensures good performance across all classes, not just majority ones\")\n",
        "\n",
        "print(\"\\n2. PERFORMANCE ANALYSIS:\")\n",
        "print(f\"   • Logistic Regression: Best balance of performance and interpretability\")\n",
        "print(f\"   • Linear SVM: Slightly faster but slightly lower performance than Logistic Regression\")\n",
        "print(f\"   • BERT: Best potential but needs more data and training time\")\n",
        "\n",
        "print(\"\\n3. PRACTICAL RECOMMENDATIONS:\")\n",
        "print(\"   • For production with limited resources: Logistic Regression\")\n",
        "print(\"   • When interpretability matters: Logistic Regression (check feature importance)\")\n",
        "print(\"   • For maximum accuracy with GPU resources: BERT\")\n",
        "print(\"   • For quick prototyping: Linear SVM\")\n",
        "\n",
        "print(\"\\n4. AREAS FOR IMPROVEMENT:\")\n",
        "print(\"   • Address class imbalance more aggressively (SMOTE, oversampling)\")\n",
        "print(\"   • Try ensemble methods (voting classifier)\")\n",
        "print(\"   • Collect more data for minority classes\")\n",
        "print(\"   • Hyperparameter tuning for all models\")"
      ],
      "metadata": {
        "id": "uzRxN4rCIub8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PREDICTION FUNCTION FOR ALL THREE MODELS\n",
        "\n",
        "def predict_with_all_models(text, show_details=True):\n",
        "    \"\"\"\n",
        "    Make predictions using all three models on a given text.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        The input text to classify\n",
        "    show_details : bool\n",
        "        Whether to print detailed analysis\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Predictions and probabilities from all models\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess the text (same as training)\n",
        "    cleaned_text = preprocess_text(text)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. LOGISTIC REGRESSION PREDICTION\n",
        "    text_tfidf = vectorizer.transform([cleaned_text])\n",
        "    lr_pred = lr_model.predict(text_tfidf)[0]\n",
        "    lr_proba = lr_model.predict_proba(text_tfidf)[0]\n",
        "    lr_confidence = max(lr_proba)\n",
        "\n",
        "    results['Logistic Regression'] = {\n",
        "        'prediction': lr_pred,\n",
        "        'confidence': lr_confidence,\n",
        "        'probabilities': dict(zip(lr_model.classes_, lr_proba))\n",
        "    }\n",
        "\n",
        "    # 2. SVM PREDICTION\n",
        "    svm_pred = svm_model.predict(text_tfidf)[0]\n",
        "\n",
        "    # For SVM, we need decision function scores\n",
        "    svm_scores = svm_model.decision_function(text_tfidf)[0]\n",
        "    # Convert scores to probabilities using softmax\n",
        "    svm_proba = np.exp(svm_scores) / np.sum(np.exp(svm_scores))\n",
        "    svm_confidence = max(svm_proba)\n",
        "\n",
        "    results['Linear SVM'] = {\n",
        "        'prediction': svm_pred,\n",
        "        'confidence': svm_confidence,\n",
        "        'probabilities': dict(zip(svm_model.classes_, svm_proba))\n",
        "    }\n",
        "\n",
        "    # 3. BERT PREDICTION\n",
        "    # Tokenize for BERT\n",
        "    bert_input = tokenizer(cleaned_text, return_tensors=\"pt\",\n",
        "                          padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    bert_input = {k: v.to(device) for k, v in bert_input.items()}\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**bert_input)\n",
        "        bert_logits = outputs.logits\n",
        "        bert_proba = torch.softmax(bert_logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    bert_pred_idx = np.argmax(bert_proba)\n",
        "    bert_pred = id2label[bert_pred_idx]\n",
        "    bert_confidence = max(bert_proba)\n",
        "\n",
        "    results['BERT'] = {\n",
        "        'prediction': bert_pred,\n",
        "        'confidence': bert_confidence,\n",
        "        'probabilities': dict(zip([id2label[i] for i in range(len(bert_proba))], bert_proba))\n",
        "    }\n",
        "\n",
        "    # Display results\n",
        "    if show_details:\n",
        "        print(\"=\"*80)\n",
        "        print(f\"INPUT TEXT: \\\"{text[:200]}...\\\"\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\nMODEL PREDICTIONS:\")\n",
        "        print(\"-\" * 60)\n",
        "        for model_name, pred_info in results.items():\n",
        "            print(f\"{model_name:20} → {pred_info['prediction']:20} (Confidence: {pred_info['confidence']:.2%})\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DETAILED ANALYSIS:\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Find agreement\n",
        "        predictions = [results[model]['prediction'] for model in results]\n",
        "        if len(set(predictions)) == 1:\n",
        "            print(\" ALL MODELS AGREE on prediction!\")\n",
        "        else:\n",
        "            print(\" MODELS DISAGREE. Most common prediction:\")\n",
        "            from collections import Counter\n",
        "            most_common = Counter(predictions).most_common(1)[0]\n",
        "            print(f\"  '{most_common[0]}' (predicted by {most_common[1]} out of 3 models)\")\n",
        "\n",
        "        # Show top 3 classes for each model\n",
        "        print(\"\\nTOP PREDICTIONS FOR EACH MODEL:\")\n",
        "        for model_name, pred_info in results.items():\n",
        "            sorted_probs = sorted(pred_info['probabilities'].items(),\n",
        "                                 key=lambda x: x[1], reverse=True)[:3]\n",
        "            print(f\"\\n{model_name}:\")\n",
        "            for cls, prob in sorted_probs:\n",
        "                print(f\"  • {cls:20} → {prob:.2%}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "wF-GSvUtJh9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TEST WITH VARIOUS EXAMPLE SENTENCES\n",
        "\n",
        "test_sentences = [\n",
        "    # Depression examples\n",
        "    \"I feel so empty and hopeless every day, nothing brings me joy anymore.\",\n",
        "\n",
        "    # Anxiety examples\n",
        "    \"My heart is racing and I can't stop worrying about everything that could go wrong.\",\n",
        "\n",
        "    # Suicidal thoughts\n",
        "    \"I don't see any reason to continue living, everything seems pointless.\",\n",
        "\n",
        "    # Normal/neutral\n",
        "    \"I had a good day today, went for a walk and finished my work on time.\",\n",
        "\n",
        "    # Stress\n",
        "    \"I'm overwhelmed with all the deadlines and responsibilities piling up.\",\n",
        "\n",
        "    # Bipolar (manic)\n",
        "    \"I haven't slept for days but I have so much energy and so many great ideas!\",\n",
        "\n",
        "    # Personality disorder (borderline traits)\n",
        "    \"One minute I love you, the next I hate you. I don't even know who I am anymore.\",\n",
        "\n",
        "    # Mixed/ambiguous\n",
        "    \"Sometimes I feel great and other times I can't get out of bed. It's confusing.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"BATCH PREDICTION TEST - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_results = {}\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEST EXAMPLE {i}:\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results = predict_with_all_models(sentence, show_details=True)\n",
        "    all_results[f\"Example {i}\"] = results"
      ],
      "metadata": {
        "id": "reu_qLkWJqhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# GENERATE SUMMARY STATISTICS\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Count agreements\n",
        "agreements = []\n",
        "predictions_by_example = []\n",
        "\n",
        "for example_name, model_results in all_results.items():\n",
        "    preds = [model_results[model]['prediction'] for model in ['Logistic Regression', 'Linear SVM', 'BERT']]\n",
        "    predictions_by_example.append(preds)\n",
        "\n",
        "    if len(set(preds)) == 1:\n",
        "        agreements.append(1)\n",
        "    else:\n",
        "        agreements.append(0)\n",
        "\n",
        "agreement_rate = sum(agreements) / len(agreements) * 100\n",
        "\n",
        "print(f\"\\n1. Model Agreement Rate: {agreement_rate:.1f}%\")\n",
        "print(f\"   • Models agreed on {sum(agreements)} out of {len(agreements)} examples\")\n",
        "\n",
        "# Calculate average confidence per model\n",
        "avg_confidences = {}\n",
        "for model_name in ['Logistic Regression', 'Linear SVM', 'BERT']:\n",
        "    confidences = []\n",
        "    for example_name, model_results in all_results.items():\n",
        "        confidences.append(model_results[model_name]['confidence'])\n",
        "    avg_confidences[model_name] = np.mean(confidences)\n",
        "\n",
        "print(\"\\n2. Average Prediction Confidence:\")\n",
        "for model_name, avg_conf in avg_confidences.items():\n",
        "    print(f\"   • {model_name:20}: {avg_conf:.2%}\")\n",
        "\n",
        "# Find most confident predictions\n",
        "print(\"\\n3. Most Confident Predictions:\")\n",
        "for example_name, model_results in all_results.items():\n",
        "    max_conf = 0\n",
        "    max_model = None\n",
        "    max_pred = None\n",
        "\n",
        "    for model_name, pred_info in model_results.items():\n",
        "        if pred_info['confidence'] > max_conf:\n",
        "            max_conf = pred_info['confidence']\n",
        "            max_model = model_name\n",
        "            max_pred = pred_info['prediction']\n",
        "\n",
        "    # Only show very confident predictions (>90%)\n",
        "    if max_conf > 0.9:\n",
        "        print(f\"   • {example_name}: {max_model} → '{max_pred}' ({max_conf:.2%})\")\n",
        "\n",
        "# Confusion analysis between models\n",
        "print(\"\\n4. Common Disagreements:\")\n",
        "disagreement_pairs = {}\n",
        "for i, preds in enumerate(predictions_by_example):\n",
        "    if len(set(preds)) > 1:\n",
        "        # Find which models disagree\n",
        "        for j in range(len(preds)):\n",
        "            for k in range(j+1, len(preds)):\n",
        "                if preds[j] != preds[k]:\n",
        "                    pair = tuple(sorted([preds[j], preds[k]]))\n",
        "                    disagreement_pairs[pair] = disagreement_pairs.get(pair, 0) + 1\n",
        "\n",
        "if disagreement_pairs:\n",
        "    for (cls1, cls2), count in sorted(disagreement_pairs.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "        print(f\"   • '{cls1}' vs '{cls2}': {count} times\")\n",
        "else:\n",
        "    print(\"   • No disagreements found in test examples\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RECOMMENDATION FOR REAL-WORLD DEPLOYMENT\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "id": "4qJ8f_zMJthO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick function for testing single sentences\n",
        "def quick_predict(text):\n",
        "    \"\"\"Simple function to get predictions from all models.\"\"\"\n",
        "    print(f\"Input: \\\"{text}\\\"\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    results = predict_with_all_models(text, show_details=False)\n",
        "\n",
        "    print(\"Predictions:\")\n",
        "    for model_name, pred_info in results.items():\n",
        "        print(f\"  • {model_name:20}: {pred_info['prediction']} ({pred_info['confidence']:.1%})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test it\n",
        "print(\"\\nQuick Test Examples:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test1 = \"I feel anxious about my future and can't stop worrying\"\n",
        "quick_predict(test1)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "test2 = \"Everything feels dark and I have no motivation\"\n",
        "quick_predict(test2)"
      ],
      "metadata": {
        "id": "at46Qt9UJ59q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save comparison results for your report\n",
        "import json\n",
        "\n",
        "# Create summary for report\n",
        "report_summary = {\n",
        "    \"dataset_info\": {\n",
        "        \"total_samples\": len(df),\n",
        "        \"classes\": list(df['status'].value_counts().index),\n",
        "        \"class_distribution\": df['status'].value_counts().to_dict()\n",
        "    },\n",
        "    \"model_performance\": comparison_df.to_dict('records'),\n",
        "    \"best_model\": {\n",
        "        \"name\": best_model_name,\n",
        "        \"test_accuracy\": float(comparison_df.loc[best_model_idx, 'Test Accuracy']),\n",
        "        \"test_macro_f1\": float(comparison_df.loc[best_model_idx, 'Test Macro F1'])\n",
        "    },\n",
        "    \"test_examples_predictions\": []\n",
        "}\n",
        "\n",
        "# Add sample predictions\n",
        "for i, sentence in enumerate(test_sentences[:3], 1):  # Just first 3 for brevity\n",
        "    results = predict_with_all_models(sentence, show_details=False)\n",
        "    example_result = {\n",
        "        \"sentence\": sentence,\n",
        "        \"predictions\": {model: info['prediction'] for model, info in results.items()}\n",
        "    }\n",
        "    report_summary[\"test_examples_predictions\"].append(example_result)\n",
        "\n",
        "# Save to JSON\n",
        "with open('model_comparison_summary.json', 'w') as f:\n",
        "    json.dump(report_summary, f, indent=2)\n",
        "\n",
        "print(\"\\n Summary saved to 'model_comparison_summary.json'\")\n",
        "print(\" Use this data for your report and presentation\")"
      ],
      "metadata": {
        "id": "4q5kK11tJ7lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlyXjsXHJ9J9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}